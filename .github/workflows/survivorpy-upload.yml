name: Upload Survivor Data to S3

on:
  workflow_dispatch:
  schedule:
    - cron: '0 8 * * 6'

jobs:
  upload_to_s3:
    runs-on: ubuntu-latest

    env:
      GITHUB_PAT: ${{ secrets.GH_PAT }}
      GITHUB_USER: ${{ secrets.GH_USER }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up R
        uses: r-lib/actions/setup-r@v2

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev cmake make g++

      - name: Set R library path and cache packages
        uses: actions/cache@v3
        with:
          path: ~/.R/library
          key: ${{ runner.os }}-r-library-${{ hashFiles('data_pipeline/run_pipeline.R') }}
          restore-keys: |
            ${{ runner.os }}-r-library-

      - name: Install R packages
        run: |
          mkdir -p ~/.R/library
          echo 'R_LIBS_USER="~/.R/library"' >> ~/.Renviron
          Rscript -e 'install.packages(c("aws.s3", "jsonlite", "remotes", "arrow", "digest"), repos="https://cloud.r-project.org", lib="~/.R/library")'
          Rscript -e 'remotes::install_github("doehm/survivoR", lib="~/.R/library")'

      - name: Run R script to upload data to S3
        run: |
          Rscript -e '.libPaths("~/.R/library"); source("data_pipeline/run_pipeline.R")'

      - name: Zip parquet files and upload archive to S3
        run: |
          mkdir -p zipped_data
          aws s3 cp s3://survivorpy-data/tables/ ./zipped_data/ --recursive --exclude "*" --include "*.parquet"
          cd zipped_data
          zip survivor_data.zip *.parquet
          aws s3 cp survivor_data.zip s3://survivorpy-data/survivor_data.zip

