name: Upload Survivor Data to S3

on:
  workflow_dispatch:

  # push:
  #   branches:
  #     - main
  #
  # schedule:
  #   - cron: '0 8 * * 6'  # Every Saturday at 1am MST (8am UTC)

jobs:
  upload_to_s3:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run R script to upload data
        run: |
          # Install base R
          sudo apt-get update
          sudo apt-get install -y r-base

          # Create a user-level R library directory and configure R to use it
          mkdir -p ~/.R/library
          echo 'R_LIBS_USER="~/.R/library"' >> ~/.Renviron

          # Install required R packages to the user library
          Rscript -e 'install.packages(c("arrow", "aws.s3", "jsonlite"), repos="https://cloud.r-project.org", lib="~/.R/library")'
          Rscript -e 'install.packages("remotes", repos="https://cloud.r-project.org", lib="~/.R/library")'
          Rscript -e 'remotes::install_github("doehm/survivoR", lib="~/.R/library")'

          # Set the library path explicitly and run the R script
          Rscript -e '.libPaths("~/.R/library"); source("scripts/convert_survivor_to_parquet.R")'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}



